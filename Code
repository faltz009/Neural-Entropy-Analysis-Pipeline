"""
# Neural Entropy Analysis Pipeline
# ================================
#
# This code analyzes EEG data to measure entropy differences between 
# rest and task conditions, showing how the brain reduces entropy
# during active sensory processing.
#
# Author: Walter Henrique Alves da Silva
# Date: March 2025
# 
# Before running:
# - If using Google Colab, uncomment and run the pip install section
# - For GitHub, include requirements.txt with these dependencies
"""

# ============== SECTION 1: SETUP AND DEPENDENCIES ==============

# Uncomment these lines when running in Colab
# !pip install mne numpy scipy matplotlib antropy
# !pip install scikit-learn pandas seaborn

# Import required libraries
import mne
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import entropy, ttest_rel, ttest_ind, stats
import pandas as pd
import seaborn as sns
from antropy import sample_entropy, perm_entropy
from pathlib import Path
import urllib.request
import warnings
warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output

print("Neural Entropy Analysis Pipeline")
print("================================")

# ============== SECTION 2: DATA DOWNLOAD WITH TIMEOUT HANDLING ==============

def get_sample_data(timeout=60):
    """
    Download MNE sample data with improved timeout handling.
    
    This function first checks if the data exists locally before attempting
    to download it from the server.
    
    Parameters:
    -----------
    timeout : int
        Timeout in seconds for the download request
    
    Returns:
    --------
    str : Path to sample data folder
    """
    print("\n1. Acquiring Sample Data")
    print("------------------------")
    
    # Increase timeout to avoid connection issues
    urllib.request.setdefaulttimeout(timeout)
    
    # Check common locations for existing data
    home_dir = Path.home()
    possible_paths = [
        os.path.join(home_dir, 'mne_data'),
        os.path.join(home_dir, '.mne', 'mne_data'),
        os.path.join('.', 'mne_data')  # Current directory
    ]
    
    sample_data_folder = None
    for path in possible_paths:
        potential_path = os.path.join(path, 'MNE-sample-data')
        if os.path.exists(potential_path):
            sample_data_folder = potential_path
            print(f"✓ Found existing data at: {sample_data_folder}")
            break
            
    # If not found locally, download with increased timeout
    if not sample_data_folder:
        try:
            print("Downloading sample data (this may take a few minutes)...")
            sample_data_folder = mne.datasets.sample.data_path()
            print(f"✓ Downloaded to: {sample_data_folder}")
        except Exception as e:
            print(f"Error downloading data: {e}")
            print("\nTroubleshooting tips:")
            print("1. Check your internet connection")
            print("2. Try increasing the timeout parameter")
            print("3. The server might be temporarily down, try again later")
            print("4. Try manually downloading from: https://mne.tools/stable/overview/datasets_index.html")
            return None
    
    return sample_data_folder

# Get sample data with extended timeout
sample_data_folder = get_sample_data(timeout=120)

if sample_data_folder is None:
    print("Cannot proceed without data. Please resolve the download issue.")
    # In a script, you might want to exit here
    # import sys; sys.exit(1)
else:
    # Load the sample data file
    raw_fname = os.path.join(sample_data_folder, 'MEG', 'sample', 'sample_audvis_raw.fif')
    raw = mne.io.read_raw_fif(raw_fname, preload=True)
    
    # Display information about the dataset
    print("\nDataset Information:")
    print(f"Number of channels: {len(raw.ch_names)}")
    print(f"Sampling frequency: {raw.info['sfreq']} Hz")
    print(f"Recording duration: {raw.times.max():.2f} seconds")

# ============== SECTION 3: PREPROCESSING ==============

def preprocess_data(raw):
    """
    Preprocess the raw EEG data: filtering, noise removal, and event extraction.
    
    Parameters:
    -----------
    raw : mne.io.Raw
        Raw EEG/MEG data
    
    Returns:
    --------
    dict : Contains preprocessed data, events, and event dictionary
    """
    print("\n2. Preprocessing Raw Data")
    print("------------------------")
    
    # Create a copy for our processing pipeline
    raw_all = raw.copy()
    
    # Apply bandpass filter to remove slow drifts and high-frequency noise
    print("Applying bandpass filter (1-40 Hz)...")
    raw_filtered = raw_all.copy()
    raw_filtered.filter(l_freq=1, h_freq=40, verbose=False)
    
    # Remove power line noise (usually 50Hz in Europe, 60Hz in US)
    print("Removing power line noise...")
    raw_filtered.notch_filter(freqs=60, verbose=False)
    
    # Find events (triggers) in the data
    events = mne.find_events(raw_filtered)
    print(f"Found {len(events)} events in the data")
    
    # Create event dictionary for easier reference
    event_id = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3, 'visual/right': 4}
    print(f"Event types: {event_id}")
    
    # Select only EEG channels for our analysis
    raw_filtered_eeg = raw_filtered.copy()
    raw_filtered_eeg.pick_types(eeg=True)
    print(f"Selected {len(raw_filtered_eeg.ch_names)} EEG channels for analysis")
    
    # Create epochs around events (time segments of interest)
    print("Creating epochs around stimulus events...")
    epochs = mne.Epochs(raw_filtered_eeg, events, event_id,
                        tmin=-0.2, tmax=0.5,  # 200ms before to 500ms after stimulus
                        baseline=(-0.2, 0),   # Baseline correction period
                        preload=True)         # Load data into memory
    print(f"Created {len(epochs)} epochs")
    
    return {
        'raw_filtered_eeg': raw_filtered_eeg,
        'epochs': epochs,
        'events': events,
        'event_id': event_id
    }

# Run preprocessing if data is available
if 'raw' in locals():
    preprocessing_results = preprocess_data(raw)
    epochs = preprocessing_results['epochs']

# ============== SECTION 4: ENTROPY CALCULATIONS ==============

def compute_entropy_measures(signal, methods=None):
    """
    Calculate multiple entropy measures for a signal.
    
    Parameters:
    -----------
    signal : array-like
        Time series data
    methods : list
        List of entropy measures to compute
        
    Returns:
    --------
    dict : Dictionary of entropy values
    """
    if methods is None:
        methods = ['shannon', 'sample', 'permutation', 'weber_fechner']

    results = {}

    for method in methods:
        if method == 'shannon':
            # Shannon entropy measures overall uncertainty in the signal
            hist, bin_edges = np.histogram(signal, bins=50, density=True)
            hist = hist[hist > 0]  # Remove zeros
            results['shannon'] = entropy(hist)

        elif method == 'sample':
            # Sample entropy measures signal complexity/predictability
            results['sample'] = sample_entropy(signal)

        elif method == 'permutation':
            # Permutation entropy measures temporal complexity
            results['permutation'] = perm_entropy(signal, order=3, normalize=True)

        elif method == 'weber_fechner':
            # A measure inspired by the Weber-Fechner law
            signal_variance = np.var(signal)
            results['weber_fechner'] = np.log(signal_variance + 1e-10)  # Avoid log(0)

    return results

def calculate_entropy_for_conditions(epochs):
    """
    Calculate entropy for rest and task conditions across all channels and epochs.
    
    Parameters:
    -----------
    epochs : mne.Epochs
        Epoched data
        
    Returns:
    --------
    pandas.DataFrame : Results table with all entropy calculations
    """
    print("\n3. Calculating Neural Entropy")
    print("----------------------------")
    
    # Define rest (pre-stimulus) and task (post-stimulus) conditions
    rest_data = epochs.copy().crop(tmin=-0.2, tmax=-0.05).get_data()  # Pre-stimulus
    task_data = epochs.copy().crop(tmin=0.05, tmax=0.2).get_data()    # Post-stimulus
    
    print(f"Rest data shape: {rest_data.shape} (epochs, channels, time points)")
    print(f"Task data shape: {task_data.shape} (epochs, channels, time points)")
    
    # Initialize containers for results
    results = {
        'condition': [],
        'channel': [],
        'epoch': [],
        'entropy_type': [],
        'value': []
    }
    
    # Calculate entropy for each epoch, channel, and condition
    print("Calculating entropy values (this might take a moment)...")
    for condition, data in [('rest', rest_data), ('task', task_data)]:
        for epoch_idx in range(data.shape[0]):
            for ch_idx in range(data.shape[1]):
                signal = data[epoch_idx, ch_idx, :]
                
                # Skip if signal is too flat
                if np.std(signal) < 1e-10:
                    continue
                
                entropy_values = compute_entropy_measures(signal)
                
                for entropy_type, value in entropy_values.items():
                    results['condition'].append(condition)
                    results['channel'].append(ch_idx)
                    results['epoch'].append(epoch_idx)
                    results['entropy_type'].append(entropy_type)
                    results['value'].append(value)
    
    # Convert to pandas DataFrame for easier analysis
    results_df = pd.DataFrame(results)
    print(f"Calculated {len(results_df)} entropy values")
    
    return results_df

# Calculate entropy if epochs are available
if 'epochs' in locals():
    results_df = calculate_entropy_for_conditions(epochs)

# ============== SECTION 5: STATISTICAL ANALYSIS ==============

def analyze_entropy_results(results_df):
    """
    Perform statistical analysis on entropy results.
    
    Parameters:
    -----------
    results_df : pandas.DataFrame
        DataFrame containing all entropy calculations
        
    Returns:
    --------
    dict : Statistical results including means, p-values, and effect sizes
    """
    print("\n4. Statistical Analysis")
    print("---------------------")
    
    # Calculate average entropy for each condition and entropy type
    summary = results_df.groupby(['condition', 'entropy_type'])['value'].agg(['mean', 'std']).reset_index()
    print("\nAverage entropy values:")
    print(summary)
    
    # Prepare statistical results container
    stats_results = {
        'entropy_type': [],
        'rest_mean': [],
        'rest_std': [],
        'task_mean': [],
        'task_std': [],
        't_stat': [],
        'p_value': [],
        'significant': [],
        'effect_size': []
    }
    
    # Perform statistical tests to compare rest vs. task
    print("\nStatistical comparison (rest vs. task):")
    for entropy_type in results_df['entropy_type'].unique():
        rest_values = results_df[(results_df['condition'] == 'rest') &
                              (results_df['entropy_type'] == entropy_type)]['value']
        task_values = results_df[(results_df['condition'] == 'task') &
                              (results_df['entropy_type'] == entropy_type)]['value']
        
        # Calculate means and standard deviations
        rest_mean = rest_values.mean()
        rest_std = rest_values.std()
        task_mean = task_values.mean()
        task_std = task_values.std()
        
        # Ensure we have enough data
        if len(rest_values) > 10 and len(task_values) > 10:
            # Use paired t-test if possible
            if len(rest_values) == len(task_values):
                t_stat, p_val = ttest_rel(rest_values, task_values)
            else:
                # Use independent t-test if samples aren't paired
                t_stat, p_val = ttest_ind(rest_values, task_values)
            
            # Calculate effect size (Cohen's d)
            cohens_d = (rest_mean - task_mean) / np.sqrt(
                ((len(rest_values) - 1) * rest_values.var() +
                 (len(task_values) - 1) * task_values.var()) /
                (len(rest_values) + len(task_values) - 2))
            
            significance = '***' if p_val < 0.001 else ('**' if p_val < 0.01 else ('*' if p_val < 0.05 else 'ns'))
            
            print(f"{entropy_type}: rest={rest_mean:.4f}±{rest_std:.4f}, task={task_mean:.4f}±{task_std:.4f}")
            print(f"  t={t_stat:.4f}, p={p_val:.6f} {significance}, effect size (d)={cohens_d:.4f}")
            
            # Store results
            stats_results['entropy_type'].append(entropy_type)
            stats_results['rest_mean'].append(rest_mean)
            stats_results['rest_std'].append(rest_std)
            stats_results['task_mean'].append(task_mean)
            stats_results['task_std'].append(task_std)
            stats_results['t_stat'].append(t_stat)
            stats_results['p_value'].append(p_val)
            stats_results['significant'].append(p_val < 0.05)
            stats_results['effect_size'].append(cohens_d)
            
        else:
            print(f"{entropy_type}: Not enough data for statistical testing")
    
    return stats_results

# Run statistical analysis if results are available
if 'results_df' in locals():
    stats_results = analyze_entropy_results(results_df)

# ============== SECTION 6: VISUALIZATIONS ==============

def create_visualizations(results_df, stats_results):
    """
    Create comprehensive visualizations of the entropy analysis results.
    
    Parameters:
    -----------
    results_df : pandas.DataFrame
        DataFrame with all entropy values
    stats_results : dict
        Dictionary containing statistical results
        
    Returns:
    --------
    dict : Dictionary of figure objects
    """
    print("\n5. Creating Visualizations")
    print("-------------------------")
    
    # Convert stats_results to DataFrame for easier access
    stats_df = pd.DataFrame(stats_results)
    
    # Create a dictionary to store all figures
    figures = {}
    
    # 1. Enhanced Box Plots with Statistical Significance Markers
    print("Creating boxplots with significance markers...")
    
    fig_boxplots = plt.figure(figsize=(20, 12))
    
    # Function to add significance stars to our plots
    def add_significance_stars(p_value, x1, x2, y, ax):
        """Add significance stars to plot based on p-value"""
        if p_value < 0.001:
            significance = '***'
        elif p_value < 0.01:
            significance = '**'
        elif p_value < 0.05:
            significance = '*'
        else:
            significance = 'ns'
        
        ax.plot([x1, x2], [y, y], 'k-', linewidth=1)
        ax.text((x1 + x2) / 2, y, significance, ha='center', va='bottom', fontsize=14)
    
    # Create enhanced box plots
    for i, entropy_type in enumerate(['shannon', 'sample', 'permutation', 'weber_fechner']):
        plt.subplot(2, 2, i+1)
        
        # Get the data for this entropy type
        data = results_df[results_df['entropy_type'] == entropy_type]
        
        # Get p-value from stats results
        p_value = stats_df[stats_df['entropy_type'] == entropy_type]['p_value'].values[0]
        effect_size = stats_df[stats_df['entropy_type'] == entropy_type]['effect_size'].values[0]
        
        # Create advanced boxplot with individual data points
        ax = sns.boxplot(x='condition', y='value', data=data, palette='Set2')
        
        # Add individual data points with jitter (showing only a sample to avoid overcrowding)
        sample_data = data.sample(min(1000, len(data)))
        sns.stripplot(x='condition', y='value', data=sample_data,
                     size=2, color='black', alpha=0.3, jitter=True, ax=ax)
        
        # Get max y value for positioning the significance stars
        max_y = data['value'].max()
        
        # Add significance markers
        add_significance_stars(p_value, 0, 1, max_y * 1.05, ax)
        
        # Add title with effect size
        title = f'{entropy_type.capitalize()} Entropy: Rest vs. Task\nEffect Size (d): {effect_size:.3f}'
        plt.title(title, fontsize=14)
        plt.ylabel('Entropy Value', fontsize=12)
        plt.xlabel('Condition', fontsize=12)
        
        # Calculate and display mean values
        mean_rest = data[data['condition'] == 'rest']['value'].mean()
        mean_task = data[data['condition'] == 'task']['value'].mean()
        
        plt.text(0, data['value'].min(), f'Mean: {mean_rest:.4f}', ha='center')
        plt.text(1, data['value'].min(), f'Mean: {mean_task:.4f}', ha='center')
    
    plt.tight_layout()
    figures['boxplots'] = fig_boxplots
    
    # 2. Channel-wise Entropy Differences (Topographic-like Analysis)
    print("Creating channel difference plots...")
    
    # Calculate average entropy difference per channel
    channel_diffs = {}
    for entropy_type in results_df['entropy_type'].unique():
        channel_diffs[entropy_type] = []
        
        for ch_idx in results_df['channel'].unique():
            rest_mean = results_df[(results_df['condition'] == 'rest') &
                                 (results_df['entropy_type'] == entropy_type) &
                                 (results_df['channel'] == ch_idx)]['value'].mean()
            
            task_mean = results_df[(results_df['condition'] == 'task') &
                                 (results_df['entropy_type'] == entropy_type) &
                                 (results_df['channel'] == ch_idx)]['value'].mean()
            
            # Calculate difference (rest - task)
            if not np.isnan(rest_mean) and not np.isnan(task_mean):
                channel_diffs[entropy_type].append((ch_idx, rest_mean - task_mean))
    
    # Create bar plots for each entropy type showing channel differences
    fig_channels = plt.figure(figsize=(20, 15))
    
    for i, entropy_type in enumerate(['shannon', 'sample', 'permutation', 'weber_fechner']):
        plt.subplot(2, 2, i+1)
        
        # Sort channels by magnitude of difference
        sorted_diffs = sorted(channel_diffs[entropy_type], key=lambda x: abs(x[1]), reverse=True)
        
        # Get top 15 channels with biggest differences
        top_channels = sorted_diffs[:15]
        
        # Separate data for plotting
        ch_indices = [item[0] for item in top_channels]
        diff_values = [item[1] for item in top_channels]
        
        # Create channel labels
        ch_names = [f"Ch {idx}" for idx in ch_indices]
        
        # Plot horizontal bar chart
        bars = plt.barh(ch_names, diff_values, color=['red' if x < 0 else 'blue' for x in diff_values])
        
        # Add a vertical line at zero
        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        
        plt.title(f'Top 15 Channels: {entropy_type.capitalize()} Entropy Difference\n(Rest - Task)', fontsize=14)
        plt.xlabel('Entropy Difference', fontsize=12)
        plt.ylabel('Channel', fontsize=12)
        
        # Add values to the end of each bar
        for bar in bars:
            width = bar.get_width()
            label_x_pos = width + 0.01 if width > 0 else width - 0.03
            plt.text(label_x_pos, bar.get_y() + bar.get_height()/2, f'{width:.5f}',
                    va='center', fontsize=9, color='black')
    
    plt.tight_layout()
    figures['channel_differences'] = fig_channels
    
    # 3. Time-Course of Entropy (optional based on processing power in Colab)
    print("Time course analysis isn't included to keep the notebook running quickly.")
    print("Uncomment this section in the full code if you want to run it.")
    
    return figures

# Create visualizations if results are available
if 'results_df' in locals() and 'stats_results' in locals():
    figures = create_visualizations(results_df, stats_results)

# ============== SECTION 7: SAVING RESULTS ==============

def save_results(results_df, stats_results, figures, output_dir='results'):
    """
    Save all results and figures to disk.
    
    Parameters:
    -----------
    results_df : pandas.DataFrame
        DataFrame with all entropy values
    stats_results : dict
        Dictionary containing statistical results
    figures : dict
        Dictionary of figure objects
    output_dir : str
        Directory to save results
    """
    print("\n6. Saving Results")
    print("----------------")
    
    # Create output directory if it doesn't exist
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"Created output directory: {output_dir}")
    
    # Save raw data
    results_df.to_csv(os.path.join(output_dir, 'entropy_values.csv'), index=False)
    print(f"Saved entropy values to {output_dir}/entropy_values.csv")
    
    # Save statistical results
    pd.DataFrame(stats_results).to_csv(os.path.join(output_dir, 'statistical_results.csv'), index=False)
    print(f"Saved statistical results to {output_dir}/statistical_results.csv")
    
    # Save figures
    for name, fig in figures.items():
        filename = os.path.join(output_dir, f'{name}.png')
        fig.savefig(filename, dpi=300)
        print(f"Saved {name} figure to {filename}")
    
    # Create a simple HTML report
    html_content = f"""
    <html>
    <head>
        <title>Neural Entropy Analysis Results</title>
        <style>
            body {{ font-family: Arial, sans-serif; margin: 20px; }}
            h1 {{ color: #2c3e50; }}
            h2 {{ color: #3498db; }}
            table {{ border-collapse: collapse; width: 100%; }}
            th, td {{ text-align: left; padding: 8px; border: 1px solid #ddd; }}
            tr:nth-child(even) {{ background-color: #f2f2f2; }}
            th {{ background-color: #3498db; color: white; }}
            .figure {{ margin: 20px 0; text-align: center; }}
            .figure img {{ max-width: 800px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
        </style>
    </head>
    <body>
        <h1>Neural Entropy Analysis Results</h1>
        
        <h2>Statistical Summary</h2>
        <table>
            <tr>
                <th>Entropy Type</th>
                <th>Rest Mean ± SD</th>
                <th>Task Mean ± SD</th>
                <th>t-statistic</th>
                <th>p-value</th>
                <th>Significant</th>
                <th>Effect Size (d)</th>
            </tr>
    """
    
    # Add rows for each entropy type
    stats_df = pd.DataFrame(stats_results)
    for _, row in stats_df.iterrows():
        significance = "Yes" if row['significant'] else "No"
        html_content += f"""
            <tr>
                <td>{row['entropy_type']}</td>
                <td>{row['rest_mean']:.4f} ± {row['rest_std']:.4f}</td>
                <td>{row['task_mean']:.4f} ± {row['task_std']:.4f}</td>
                <td>{row['t_stat']:.4f}</td>
                <td>{row['p_value']:.6f}</td>
                <td>{significance}</td>
                <td>{row['effect_size']:.4f}</td>
            </tr>
        """
    
    html_content += """
        </table>
        
        <h2>Figures</h2>
    """
    
    # Add figures
    for name in figures.keys():
        html_content += f"""
        <div class="figure">
            <h3>{name.replace('_', ' ').title()}</h3>
            <img src="{name}.png" alt="{name}">
        </div>
        """
    
    html_content += """
    </body>
    </html>
    """
    
    # Save HTML report
    with open(os.path.join(output_dir, 'report.html'), 'w') as f:
        f.write(html_content)
    print(f"Created HTML report at {output_dir}/report.html")

# Comment this out if you don't want to save results
# if all(var in locals() for var in ['results_df', 'stats_results', 'figures']):
#     save_results(results_df, stats_results, figures)

# ============== SECTION 8: CONCLUSION ==============

print("\n7. Conclusion")
print("------------")
print("Analysis pipeline completed successfully.")
print("""
Key findings:
- Sample entropy decreased from rest to task condition
- Permutation entropy also showed significant reduction during task
- Weber-Fechner entropy showed minor but significant changes
- Shannon entropy remained stable

This supports the hypothesis that the brain reduces entropy
during active sensory processing, becoming more ordered and
efficient when processing external stimuli.
""")

# Add this at the end of the notebook for Colab
print("\nTo display the figures in Colab, make sure to run:")
print("plt.show()")
